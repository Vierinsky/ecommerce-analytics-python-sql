{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6165b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import hashlib\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy.types import Text, DateTime, Numeric\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a23397",
   "metadata": {},
   "source": [
    "Loading environment variables (.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 1) Load environment variables (.env) --------\n",
    "\n",
    "# This allows to change credentials without changing code.\n",
    "load_dotenv()\n",
    "\n",
    "DB_USER = os.getenv(\"DB_USER\", \"postgres\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\", \"\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"ecom\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\n",
    "DB_PORT = int(os.getenv(\"DB_PORT\", \"5432\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ce048",
   "metadata": {},
   "source": [
    "Building the SQLAlchemy engine URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a83774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 2) Build the SQLAlchemy engine URL --------\n",
    "\n",
    "# Connecting to \"ecom\" DB\n",
    "url = URL.create(\n",
    "    \"postgresql+psycopg2\",\n",
    "    username=DB_USER,\n",
    "    password=DB_PASS,\n",
    "    host=DB_HOST,\n",
    "    port=DB_PORT,\n",
    "    database=DB_NAME\n",
    ")\n",
    "\n",
    "# Creating engine\n",
    "engine = create_engine(url, echo=False, pool_pre_ping=True, future=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c1ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 3) Load the CSV with pandas --------\n",
    "\n",
    "# Creating a relative path to sample_orders.csv\n",
    "csv_path = Path.cwd() / \"data\" / \"sample_orders.csv\"\n",
    "\n",
    "# Mapping date columns' names\n",
    "date_cols = [\n",
    "    \"order_purchase_timestamp\",\n",
    "    \"order_approved_at\",\n",
    "    \"order_delivered_customer_date\",\n",
    "    \"order_estimated_delivery_date\"\n",
    "]\n",
    "\n",
    "# Mapping expected columns\n",
    "expected_cols = [\n",
    "    \"order_id\",\n",
    "    \"customer_id\",\n",
    "    \"order_status\",\n",
    "    # \"order_purchase_timestamp\",   # Let's evaluate if we end up incorporating this column.\n",
    "    \"order_approved_at\",\n",
    "    \"order_delivered_customer_date\",\n",
    "    \"order_estimated_delivery_date\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70f058c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>order_delivered_customer_date</th>\n",
       "      <th>order_estimated_delivery_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6961b52186094adeb7a6a1b4c6e45e72</td>\n",
       "      <td>6a46bdaf797f47a8bd992c27dda6e467</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-05-12 15:46:46</td>\n",
       "      <td>2018-05-12 17:33:46</td>\n",
       "      <td>2018-05-27 14:33:46</td>\n",
       "      <td>2018-05-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5fdd8858fb3f4aa098c1a06da5138384</td>\n",
       "      <td>eee1fe9e53424b25b03acb15183a0a39</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-03-28 11:21:41</td>\n",
       "      <td>2017-03-28 13:27:41</td>\n",
       "      <td>2017-04-11 21:27:41</td>\n",
       "      <td>2017-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>491734820cce40348c2a6ea7612ff131</td>\n",
       "      <td>b9c1d556ad274173b092bb63afaef30c</td>\n",
       "      <td>shipped</td>\n",
       "      <td>2017-01-20 10:14:03</td>\n",
       "      <td>2017-01-20 12:02:03</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c947f49479f940bd959ce12f75585351</td>\n",
       "      <td>bd84e2305cef4a2c907c577b2c950199</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-07-30 23:30:11</td>\n",
       "      <td>2018-07-31 00:12:11</td>\n",
       "      <td>2018-08-03 07:12:11</td>\n",
       "      <td>2018-08-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f0acbf1378324a4b8188adef6b72c143</td>\n",
       "      <td>6cba08cfe2a247daaa37257083bd8ebb</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-08-02 14:55:05</td>\n",
       "      <td>2017-08-02 16:28:05</td>\n",
       "      <td>2017-08-15 16:28:05</td>\n",
       "      <td>2017-08-21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id                       customer_id  \\\n",
       "0  6961b52186094adeb7a6a1b4c6e45e72  6a46bdaf797f47a8bd992c27dda6e467   \n",
       "1  5fdd8858fb3f4aa098c1a06da5138384  eee1fe9e53424b25b03acb15183a0a39   \n",
       "2  491734820cce40348c2a6ea7612ff131  b9c1d556ad274173b092bb63afaef30c   \n",
       "3  c947f49479f940bd959ce12f75585351  bd84e2305cef4a2c907c577b2c950199   \n",
       "4  f0acbf1378324a4b8188adef6b72c143  6cba08cfe2a247daaa37257083bd8ebb   \n",
       "\n",
       "  order_status order_purchase_timestamp   order_approved_at  \\\n",
       "0    delivered      2018-05-12 15:46:46 2018-05-12 17:33:46   \n",
       "1    delivered      2017-03-28 11:21:41 2017-03-28 13:27:41   \n",
       "2      shipped      2017-01-20 10:14:03 2017-01-20 12:02:03   \n",
       "3    delivered      2018-07-30 23:30:11 2018-07-31 00:12:11   \n",
       "4    delivered      2017-08-02 14:55:05 2017-08-02 16:28:05   \n",
       "\n",
       "  order_delivered_customer_date order_estimated_delivery_date  \n",
       "0           2018-05-27 14:33:46                    2018-05-29  \n",
       "1           2017-04-11 21:27:41                    2017-04-09  \n",
       "2                           NaT                    2017-02-01  \n",
       "3           2018-08-03 07:12:11                    2018-08-13  \n",
       "4           2017-08-15 16:28:05                    2017-08-21  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a Pandas df\n",
    "\n",
    "sample_orders = pd.read_csv(\n",
    "    csv_path, \n",
    "    parse_dates=date_cols, \n",
    "    keep_default_na=True,    # empty strings -> NaN\n",
    "    usecols=expected_cols\n",
    "    )\n",
    "\n",
    "sample_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f18366c",
   "metadata": {},
   "source": [
    "Adding operational columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 4) Add operational columns --------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd644e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ingested_at = when we loaded the row\n",
    "sample_orders[\"_ingested_at\"] = pd.Timestamp.utcnow()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5ffdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _source_file = Which file fed this row (helps trace issues later)\n",
    "sample_orders[\"_source_file\"] = os.path.basename(csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e974d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _row_md5 = simple row-level checksum for idempotency/dedup in staging\n",
    "# We hash a stable concatenation of key fields.\n",
    "\n",
    "def row_hash(row) -> str:\n",
    "    # We use natural keys + important fields that define the row's identity\n",
    "    parts = [\n",
    "        str(row[\"order_id\"]),\n",
    "        str(row[\"customer_id\"]),\n",
    "        str(row[\"order_status\"]),\n",
    "        # We use ISD format for datetimes to make deterministic strings\n",
    "        row[\"order_approved_at\"].isoformat() if pd.notna(row[\"order_approved_at\"]) else \"\",\n",
    "        row[\"order_delivered_customer_date\"].isoformat() if pd.notna(row[\"order_delivered_customer_date\"]) else \"\",\n",
    "        row[\"order_estimated_delivery_date\"].isoformat() if pd.notna(row[\"order_estimated_delivery_date\"]) else \"\",\n",
    "    ]\n",
    "    txt = \"|\".join(parts)\n",
    "    return hashlib.md5(txt.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "sample_orders[\"_row_md5\"] = sample_orders.apply(row_hash, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5d8b2",
   "metadata": {},
   "source": [
    "Map pandas dtypes to SQL column types (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d70ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 5) Map pandas dtypes to SQL column types (optional) --------\n",
    "# This helps Postgres store correct types. Matches staging.orders_raw DDL.\n",
    "\n",
    "dtype_map = {\n",
    "    \"order_id\": Text(),\n",
    "    \"customer_id\" : Text(),\n",
    "    \"order_status\"  : Text(),\n",
    "    # \"order_purchase_timestamp\" : DateTime(),\n",
    "    \"order_approved_at\" : DateTime(),\n",
    "    \"order_delivered_customer_date\" : DateTime(),\n",
    "    \"order_estimated_delivery_date\" : DateTime(),\n",
    "    \"_ingested_at\" : DateTime(),\n",
    "    \"_source_file\" : Text(),\n",
    "    \"_row_md5\" : Text()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a55654",
   "metadata": {},
   "source": [
    " to Postgres (staging.orders_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc5e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 6) Write to Postgres (staging.orders_raw) --------\n",
    "# if_exists='append' so we can run it multiple times; let's use chunksize for large files\n",
    "\n",
    "table_name = \"orders_raw\"\n",
    "schema_name = \"staging\"\n",
    "\n",
    "with engine.begin() as conn: # begin() = transaction; commits or rolls back automatically\n",
    "    # Optional: simple dedup strategy per file â€” delete rows with same _source_file first\n",
    "    # This makes reruns idempotent for the same CSV. Comment out if you prefer pure append.\n",
    "\n",
    "    conn.exec_driver_sql(\n",
    "        f\"DELETE FROM {schema_name}.{table_name} WHERE _source_file = %(src)s\",\n",
    "        {\"src\": os.path.basename(csv_path)},\n",
    "    )\n",
    "\n",
    "    sample_orders.to_sql(\n",
    "        name=table_name,\n",
    "        con=conn,\n",
    "        schema=schema_name,\n",
    "        if_exists=\"append\"\n",
    "        index=False,\n",
    "        dtype=dtype_map,\n",
    "        method=\"multi\",     # batched inserts\n",
    "        chunksize=1_000,    # can be adjusted\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a16312",
   "metadata": {},
   "source": [
    "Quick verification query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958373fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 7) Quick verification query --------\n",
    "with engine.connect() as conn:\n",
    "    res = conn.exec_driver_sql(\"SELECT COUNT(*) FROM staging.orders_raw;\")\n",
    "    count = res.scalar_one()\n",
    "    print(f\"Loaded rows in staging.orders_raw: {count}\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
